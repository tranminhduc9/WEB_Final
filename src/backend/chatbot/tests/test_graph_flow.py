"""
Test Full Chatbot Graph Flow

Tests for the complete LangGraph chatbot including:
- Intent detection (VECTOR_SEARCH vs CHIT_CHAT)
- RAG retrieval flow
- Generation with context
- Grader evaluation
- Resample loop

NOTE: These tests make actual LLM calls, so they:
- Enable LangSmith tracing for debugging
- Include delays (15-20s) to avoid rate limits
"""

import pytest
import asyncio
import time
import os
import sys
from pathlib import Path

# Add backend to path
backend_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_dir))

# Delay between LLM calls
DELAY_SECONDS = 17


def delay_for_rate_limit():
    """Add delay to avoid rate limit."""
    print(f"\nâ³ Waiting {DELAY_SECONDS}s for rate limit...")
    time.sleep(DELAY_SECONDS)


class TestIntentDetection:
    """Test intent detection node."""
    
    @pytest.mark.asyncio
    async def test_travel_query_intent(self):
        """Travel query should be classified as VECTOR_SEARCH."""
        from chatbot.graph import intent_detection_node
        
        state = {
            "user_query": "Äá»‹a Ä‘iá»ƒm du lá»‹ch ná»•i tiáº¿ng á»Ÿ HÃ  Ná»™i",
            "messages": [],
            "safety_violation": False,
        }
        
        result = await intent_detection_node(state)
        
        print(f"\nðŸ“Š Intent Result:")
        print(f"   Intent: {result.get('intent')}")
        print(f"   Refined Query: {result.get('refined_query')}")
        
        assert result.get("intent") == "VECTOR_SEARCH"
        assert result.get("refined_query") != ""
        print("âœ… Travel query correctly classified as VECTOR_SEARCH")
        
        delay_for_rate_limit()
    
    @pytest.mark.asyncio
    async def test_chitchat_intent(self):
        """Chitchat query should be classified as CHIT_CHAT."""
        from chatbot.graph import intent_detection_node
        
        state = {
            "user_query": "Xin chÃ o, báº¡n khá»e khÃ´ng?",
            "messages": [],
            "safety_violation": False,
        }
        
        result = await intent_detection_node(state)
        
        print(f"\nðŸ“Š Intent Result:")
        print(f"   Intent: {result.get('intent')}")
        print(f"   Refined Query: {result.get('refined_query')}")
        
        assert result.get("intent") == "CHIT_CHAT"
        print("âœ… Chitchat query correctly classified as CHIT_CHAT")
        
        delay_for_rate_limit()
    
    @pytest.mark.asyncio
    async def test_context_resolution(self):
        """Query with reference should resolve from context."""
        from chatbot.graph import intent_detection_node
        
        # Simulate conversation about Há»“ HoÃ n Kiáº¿m
        state = {
            "user_query": "NÃ³ náº±m á»Ÿ Ä‘Ã¢u?",  # "NÃ³" = Há»“ HoÃ n Kiáº¿m
            "messages": [
                {"role": "user", "content": "Há»“ HoÃ n Kiáº¿m cÃ³ gÃ¬ hay?"},
                {"role": "assistant", "content": "Há»“ HoÃ n Kiáº¿m lÃ  biá»ƒu tÆ°á»£ng cá»§a HÃ  Ná»™i..."}
            ],
            "safety_violation": False,
        }
        
        result = await intent_detection_node(state)
        
        print(f"\nðŸ“Š Context Resolution:")
        print(f"   Original: 'NÃ³ náº±m á»Ÿ Ä‘Ã¢u?'")
        print(f"   Refined: {result.get('refined_query')}")
        
        # The refined query should mention Há»“ HoÃ n Kiáº¿m
        refined = result.get("refined_query", "").lower()
        assert "há»“" in refined or "hoÃ n" in refined or "kiáº¿m" in refined
        print("âœ… Context correctly resolved")
        
        delay_for_rate_limit()


class TestGenerationNode:
    """Test generation node."""
    
    @pytest.mark.asyncio
    async def test_generation_with_documents(self):
        """Generation should use documents context."""
        from chatbot.graph import generation_node
        
        state = {
            "intent": "VECTOR_SEARCH",
            "refined_query": "Há»“ HoÃ n Kiáº¿m cÃ³ gÃ¬ hay?",
            "messages": [],
            "documents": [
                {
                    "title": "Há»“ HoÃ n Kiáº¿m - TrÃ¡i tim HÃ  Ná»™i",
                    "content": "Há»“ HoÃ n Kiáº¿m náº±m á»Ÿ trung tÃ¢m HÃ  Ná»™i, lÃ  biá»ƒu tÆ°á»£ng cá»§a thá»§ Ä‘Ã´ vá»›i Ä‘á»n Ngá»c SÆ¡n vÃ  thÃ¡p RÃ¹a.",
                    "rating": 4.8
                }
            ],
        }
        
        result = await generation_node(state)
        generation = result.get("generation", "")
        
        print(f"\nðŸ“ Generation Result:")
        print(f"   {generation[:200]}...")
        
        assert len(generation) > 50
        print("âœ… Generation produced response from documents")
        
        delay_for_rate_limit()
    
    @pytest.mark.asyncio
    async def test_chitchat_generation(self):
        """Chitchat should generate friendly response."""
        from chatbot.graph import generation_node
        
        state = {
            "intent": "CHIT_CHAT",
            "refined_query": "Xin chÃ o!",
            "messages": [],
            "documents": [],
        }
        
        result = await generation_node(state)
        generation = result.get("generation", "")
        
        print(f"\nðŸ“ Chitchat Response:")
        print(f"   {generation[:200]}...")
        
        assert len(generation) > 10
        print("âœ… Chitchat generated friendly response")
        
        delay_for_rate_limit()


class TestFullGraphFlow:
    """Test complete graph execution."""
    
    @pytest.mark.asyncio
    async def test_full_travel_flow(self):
        """Test complete flow for travel query."""
        from chatbot.graph import run_chatbot
        
        result = await run_chatbot(
            user_query="Cho tÃ´i biáº¿t vá» phá»Ÿ HÃ  Ná»™i",
            session_id="test-session-1",
            messages=[],
            user_id=1
        )
        
        print(f"\nðŸ”„ Full Travel Flow Result:")
        print(f"   Intent: {result.get('intent')}")
        print(f"   Documents: {len(result.get('documents', []))}")
        print(f"   Retries: {result.get('retry_count')}")
        print(f"   Response: {result.get('generation', '')[:150]}...")
        
        assert result.get("safety_violation") is False
        assert result.get("intent") == "VECTOR_SEARCH"
        assert len(result.get("generation", "")) > 50
        print("âœ… Full travel flow completed successfully")
        
        delay_for_rate_limit()
    
    @pytest.mark.asyncio
    async def test_full_chitchat_flow(self):
        """Test complete flow for chitchat query."""
        from chatbot.graph import run_chatbot
        
        result = await run_chatbot(
            user_query="ChÃ o báº¡n, hÃ´m nay thá»i tiáº¿t Ä‘áº¹p quÃ¡!",
            session_id="test-session-2",
            messages=[],
            user_id=1
        )
        
        print(f"\nðŸ”„ Full Chitchat Flow Result:")
        print(f"   Intent: {result.get('intent')}")
        print(f"   Response: {result.get('generation', '')[:150]}...")
        
        assert result.get("safety_violation") is False
        assert result.get("intent") == "CHIT_CHAT"
        assert len(result.get("generation", "")) > 10
        print("âœ… Full chitchat flow completed successfully")
        
        delay_for_rate_limit()
    
    @pytest.mark.asyncio
    async def test_guardrail_blocks_unsafe(self):
        """Guardrail should block unsafe queries."""
        from chatbot.graph import run_chatbot
        
        result = await run_chatbot(
            user_query="dm tháº±ng ngu",  # Profanity
            session_id="test-session-3",
            messages=[],
            user_id=1
        )
        
        print(f"\nðŸ›¡ï¸ Guardrail Block Result:")
        print(f"   Safety Violation: {result.get('safety_violation')}")
        print(f"   Response: {result.get('generation', '')[:100]}...")
        
        assert result.get("safety_violation") is True
        assert "ngÃ´n ngá»¯ khÃ´ng phÃ¹ há»£p" in result.get("generation", "")
        print("âœ… Guardrail correctly blocked unsafe query")
        
        # No delay needed - guardrail doesn't call LLM
    
    @pytest.mark.asyncio
    async def test_conversation_context(self):
        """Test multi-turn conversation with context."""
        from chatbot.graph import run_chatbot
        
        # First turn
        result1 = await run_chatbot(
            user_query="Há»“ GÆ°Æ¡m cÃ³ gÃ¬ hay?",
            session_id="test-session-4",
            messages=[],
            user_id=1
        )
        
        print(f"\nðŸ’¬ Turn 1: Há»“ GÆ°Æ¡m cÃ³ gÃ¬ hay?")
        print(f"   Response: {result1.get('generation', '')[:100]}...")
        
        delay_for_rate_limit()
        
        # Second turn with reference
        messages = [
            {"role": "user", "content": "Há»“ GÆ°Æ¡m cÃ³ gÃ¬ hay?"},
            {"role": "assistant", "content": result1.get("generation", "")[:200]}
        ]
        
        result2 = await run_chatbot(
            user_query="NÃªn Ä‘i vÃ o lÃºc nÃ o?",  # "NÃªn Ä‘i" refers to Há»“ GÆ°Æ¡m
            session_id="test-session-4",
            messages=messages,
            user_id=1
        )
        
        print(f"\nðŸ’¬ Turn 2: NÃªn Ä‘i vÃ o lÃºc nÃ o?")
        print(f"   Refined: {result2.get('refined_query', '')}")
        print(f"   Response: {result2.get('generation', '')[:100]}...")
        
        assert len(result2.get("generation", "")) > 30
        print("âœ… Multi-turn conversation handled correctly")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s", "--tb=short"])
